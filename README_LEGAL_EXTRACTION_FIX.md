# Legal Document Extraction - ENAMETOOLONG Fix

## Problem Description

The legal document extraction endpoint was experiencing `ENAMETOOLONG` errors when processing large PDF documents. This error occurs on Windows when command line arguments exceed the maximum allowed length (approximately 32,767 characters).

## Root Cause

The issue was in `server.js` line 481, where the entire extracted text from PDF documents was being passed as a command line argument to the Python script:

```javascript
const pythonProcess = spawn(getPythonPath(), ['scripts/legal_document_extractor.py', '--extract', text]);
```

When PDF documents contain large amounts of text, the command line becomes too long for Windows to handle, resulting in the `ENAMETOOLONG` error.

## Solution

### 1. File-Based Communication

Instead of passing text through command line arguments, we now use temporary files:

- **Before**: `spawn(python, ['script.py', '--extract', largeText])`
- **After**: `spawn(python, ['script.py', '--file', tempFilePath])`

### 2. Temporary File Management

- Create unique temporary files with timestamps and random strings
- Ensure proper cleanup in all scenarios (success, error, timeout)
- Added `temp/` directory to `.gitignore`

### 3. Python Script Updates

Updated both the original and simplified extractors to support file input:

```python
parser.add_argument('--file', type=str, help='File path containing text to extract metadata from')
```

### 4. Simplified Extractor

Created `scripts/legal_document_extractor_simple.py` that doesn't require spaCy for easier testing and deployment.

## Files Modified

### Server Changes (`server.js`)
- **Lines 481-520**: Updated `extractLegalMetadata` function to use file-based communication
- Added temporary file creation and cleanup
- Added error handling for file operations

### Python Script Changes
- **`scripts/legal_document_extractor.py`**: Added `--file` argument support
- **`scripts/legal_document_extractor_simple.py`**: New simplified version without spaCy dependency

### Configuration Changes
- **`.gitignore`**: Added `temp/` directory to prevent temporary files from being committed

## Testing

### Test Scripts
- **`test_legal_extractor_fix.js`**: Tests the file-based approach with large text
- **`test_server_endpoint.js`**: Tests the complete server endpoint

### Test Results
```
âœ… Test passed! Legal extractor works with large text.
Text length: 28,447 characters
ðŸŽ‰ All tests passed!
```

## Usage

### Server Endpoint
```bash
# Start the server
node server.js

# Test with large text
curl -X POST http://localhost:3000/extract/legal \
  -H "Content-Type: application/json" \
  -d '{"text": "large legal document text..."}'
```

### Direct Python Usage
```bash
# Using file input (recommended for large text)
python scripts/legal_document_extractor_simple.py --file temp_document.txt

# Using direct text input (for small text only)
python scripts/legal_document_extractor_simple.py --extract "small text"
```

## Benefits

1. **No More ENAMETOOLONG Errors**: Handles documents of any size
2. **Better Performance**: File I/O is more efficient than command line arguments
3. **Improved Reliability**: Proper error handling and cleanup
4. **Cross-Platform**: Works on Windows, Linux, and macOS
5. **Simplified Deployment**: No spaCy dependency required for basic functionality

## Dependencies

### Required
- Node.js (for server)
- Python 3.7+ (for extraction)

### Optional
- spaCy (for advanced NLP features in the full extractor)

## Troubleshooting

### Common Issues

1. **Temp Directory Not Found**
   ```bash
   # The server will create the temp directory automatically
   # If issues persist, create manually:
   mkdir temp
   ```

2. **Python Path Issues**
   ```bash
   # Ensure Python is in PATH or update getPythonPath() in server.js
   ```

3. **File Permission Issues**
   ```bash
   # Ensure write permissions for temp directory
   chmod 755 temp/
   ```

### Debug Mode

Enable detailed error logging:
```bash
NODE_ENV=development node server.js
```

## Performance Considerations

- **Memory Usage**: Large files are read into memory once
- **Disk Usage**: Temporary files are cleaned up automatically
- **Processing Time**: Scales linearly with document size
- **Concurrent Requests**: Each request uses a unique temporary file

## Security Considerations

- Temporary files are created with unique names to prevent conflicts
- Files are cleaned up even if the process crashes
- No sensitive data is logged in production mode
- File paths are validated before processing

## Future Improvements

1. **Streaming**: Process documents in chunks for very large files
2. **Caching**: Cache extraction results for repeated documents
3. **Async Processing**: Queue large documents for background processing
4. **Compression**: Compress temporary files for very large documents 